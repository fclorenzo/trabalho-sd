# trabalho-sd

# ğŸ›ï¸ Arquitetura da SoluÃ§Ã£o

A arquitetura deste projeto foi desenhada para ser moderna, robusta e, acima de tudo, modular, permitindo uma comparaÃ§Ã£o de desempenho justa e clara entre as tecnologias de banco de dados SQL e NoSQL. A soluÃ§Ã£o segue o padrÃ£o clÃ¡ssico **Arquitetura em 3 Camadas (3-Tier Architecture)**, com cada componente containerizado via Docker para garantir consistÃªncia e reprodutibilidade do ambiente.

## VisÃ£o Geral

O sistema consiste em duas implementaÃ§Ãµes de back-end funcionalmente idÃªnticas. Ambas expÃµem uma API REST para operaÃ§Ãµes CRUD (Create, Read, Update, Delete) em uma entidade `Produto`, mas diferem unicamente na camada de persistÃªncia de dados:

1. **Stack 1:** API em **Nest.js** conectada a um banco de dados relacional **PostgreSQL**.
2. **Stack 2:** API em **Nest.js** conectada a um banco de dados nÃ£o-relacional (NoSQL) **MongoDB**.

Essa abordagem de espelhamento arquitetÃ´nico permite isolar o banco de dados como a principal variÃ¡vel nos experimentos de desempenho.

## As 3 Camadas

### 1\. Camada de Cliente (Simulada)

NÃ£o hÃ¡ uma interface de usuÃ¡rio (front-end) neste projeto. A camada de cliente Ã© simulada pela ferramenta **Apache JMeter**.

* **Responsabilidade:** Gerar cargas de trabalho massivas e simultÃ¢neas contra os endpoints da API, simulando o comportamento de centenas de usuÃ¡rios.
* **Justificativa:** JMeter Ã© uma ferramenta padrÃ£o da indÃºstria para testes de performance, permitindo a configuraÃ§Ã£o detalhada de cenÃ¡rios de teste, como a proporÃ§Ã£o de operaÃ§Ãµes de leitura e escrita (50/50, 75/25, 25/75), e a coleta de mÃ©tricas essenciais como vazÃ£o (throughput) e tempo de resposta.

### 2\. Camada de AplicaÃ§Ã£o (Servidor Web)

O coraÃ§Ã£o da nossa aplicaÃ§Ã£o, responsÃ¡vel por toda a lÃ³gica de negÃ³cio.

* **Tecnologia:** **Nest.js** (um framework Node.js construÃ­do com TypeScript).
* **Responsabilidade:**
  * Expor os endpoints da API REST (`/produtos`).
  * Receber e validar as requisiÃ§Ãµes HTTP do cliente (JMeter).
  * Orquestrar as operaÃ§Ãµes de negÃ³cio (que neste caso sÃ£o as prÃ³prias operaÃ§Ãµes CRUD).
  * Comunicar-se com a camada de dados para persistir e recuperar informaÃ§Ãµes.
* **CaracterÃ­sticas Chave:** A API Ã© projetada para ser **stateless**. Isso significa que nenhuma informaÃ§Ã£o de sessÃ£o do cliente Ã© mantida no servidor entre as requisiÃ§Ãµes. Cada requisiÃ§Ã£o Ã© tratada de forma independente, uma premissa fundamental para sistemas distribuÃ­dos escalÃ¡veis e tolerantes a falhas.
* **Justificativa:** Nest.js foi escolhido por sua arquitetura modular e opinativa, que promove um cÃ³digo limpo e organizado. O uso de TypeScript adiciona uma camada de seguranÃ§a de tipos, reduzindo a probabilidade de erros em tempo de execuÃ§Ã£o, enquanto sua performance em operaÃ§Ãµes I/O-bound (como esperar o banco de dados responder) Ã© excelente para APIs.

### 3\. Camada de Dados (Banco de Dados)

Esta Ã© a camada onde reside a principal variÃ¡vel do nosso experimento.

* **Responsabilidade:** Armazenamento, gerenciamento e recuperaÃ§Ã£o dos dados da aplicaÃ§Ã£o de forma eficiente e confiÃ¡vel.
* **VariaÃ§Ã£o 1 (SQL): PostgreSQL**
  * **Justificativa:** Um dos sistemas de gerenciamento de banco de dados relacional mais avanÃ§ados e robustos do mundo. Foi escolhido por sua conformidade com o padrÃ£o ACID, sua performance consistente em leituras complexas e seu ecossistema maduro. Representa a abordagem tradicional e estruturada para persistÃªncia de dados.
* **VariaÃ§Ã£o 2 (NoSQL): MongoDB**
  * **Justificativa:** Um banco de dados orientado a documentos, lÃ­der na categoria NoSQL. Foi escolhido por sua flexibilidade de schema, escalabilidade horizontal e alta performance em operaÃ§Ãµes de escrita intensiva. Representa a abordagem moderna e flexÃ­vel, otimizada para grandes volumes de dados e agilidade no desenvolvimento.

## ğŸ“¦ ContainerizaÃ§Ã£o com Docker

Toda a infraestrutura da aplicaÃ§Ã£o (tanto para a stack Postgres quanto para a Mongo) Ã© gerenciada pelo **Docker** e **Docker Compose**.

* **`Dockerfile`:** Cada API possui seu prÃ³prio `Dockerfile`, que contÃ©m a "receita" para construir uma imagem de contÃªiner leve e otimizada para produÃ§Ã£o.
* **`docker-compose.yml`:** Cada stack tem um arquivo de orquestraÃ§Ã£o que, com um Ãºnico comando (`docker-compose up`), sobe dois serviÃ§os interconectados: o contÃªiner da aplicaÃ§Ã£o Nest.js e o contÃªiner do respectivo banco de dados (Postgres ou Mongo).
* **Justificativa:** O uso de Docker resolve o clÃ¡ssico problema "na minha mÃ¡quina funciona". Ele garante que todos os membros da equipe e o ambiente de testes executem a aplicaÃ§Ã£o exatamente da mesma forma, eliminando inconsistÃªncias de ambiente e facilitando a configuraÃ§Ã£o e a execuÃ§Ã£o dos experimentos.

## ğŸ“Š Diagrama da Arquitetura

O diagrama abaixo ilustra o fluxo de requisiÃ§Ãµes e a estrutura paralela das duas stacks sob teste.

```mermaid
graph TD
    subgraph "Ambiente de Teste"
        JMeter[ğŸ“Š Apache JMeter]
    end

    subgraph "Stack 1: SQL"
        API_PG[ğŸš€ API Nest.js<br>(Porta 3000)]
        DB_PG[(ğŸ—„ï¸ PostgreSQL)]
    end

    subgraph "Stack 2: NoSQL"
        API_MONGO[ğŸš€ API Nest.js<br>(Porta 3001)]
        DB_MONGO[(ğŸƒ MongoDB)]
    end

    JMeter --"RequisiÃ§Ãµes HTTP<br>(Ex: POST /produtos)"--> API_PG
    API_PG <--> DB_PG

    JMeter --"RequisiÃ§Ãµes HTTP<br>(Ex: POST /produtos)"--> API_MONGO
    API_MONGO <--> DB_MONGO
```
# ğŸ“š Guia RÃ¡pido - Trabalho de Sistemas DistribuÃ­dos

## ğŸš€ Para Executar o Trabalho (3 comandos)

```powershell
# 1. Subir containers (execute em cada pasta)
cd api-postgres
docker-compose up -d

cd ..\api-mongo  
docker-compose up -d

# 2. Instalar JMeter (se nÃ£o tiver)
# Baixe: https://jmeter.apache.org/download_jmeter.cgi
# Extraia para: C:\apache-jmeter\

# 3. Executar trabalho completo
cd ..\scripts
.\run-trabalho-completo.ps1
```

**â±ï¸ Tempo total:** ~50-60 minutos

---

## ğŸ“ Arquivos Criados

### Scripts (`scripts/`)
- **`run-trabalho-completo.ps1`** â­ - Script principal do trabalho
  - Popula 50k registros
  - Executa 3 cenÃ¡rios (A, B, C) 
  - Testa com 20k para comparaÃ§Ã£o
  - Gera todos os relatÃ³rios

- **`seed-data.ps1`** / **`seed-data.js`** - Popular banco com dados
  - Windows/PowerShell: `.\seed-data.ps1 -ApiUrl "http://localhost:3000" -Count 50000`
  - macOS/Linux/Node.js: `node seed-data.js http://localhost:3000 50000`

- **`quick-test.ps1`** - Teste individual rÃ¡pido
  - Uso: `.\quick-test.ps1 -Api "postgres" -Scenario "balanced"`

- **`generate-summary.ps1`** - Gera resumo comparativo
  - Chamado automaticamente pelo script principal

- **`demo-report.ps1`** - Demo rÃ¡pida (30s) para ver como funciona
  - Uso: `.\demo-report.ps1`

### Planos JMeter (`jmeter/`)
- **`test-plan-balanced.jmx`** - CenÃ¡rio A (50% leitura / 50% escrita)
- **`test-plan-read-heavy.jmx`** - CenÃ¡rio B (75% leitura / 25% escrita)
- **`test-plan-write-heavy.jmx`** - CenÃ¡rio C (25% leitura / 75% escrita)

### DocumentaÃ§Ã£o
- **`TRABALHO.md`** â­ - Guia completo do trabalho
- **`GUIA_JMETER.md`** - Tutorial detalhado do JMeter
- **`README.md`** - DocumentaÃ§Ã£o da arquitetura

---

## ğŸ“Š O que o Script Faz

1. âœ… Verifica prÃ©-requisitos (JMeter, APIs)
2. ğŸŒ± Popula PostgreSQL com 50.000 produtos
3. ğŸŒ± Popula MongoDB com 50.000 produtos
4. ğŸš€ Executa **6 testes** (3 cenÃ¡rios Ã— 2 bancos):
   - CenÃ¡rio A: 50 clientes lendo + 50 escrevendo (5 min)
   - CenÃ¡rio B: 75 clientes lendo + 25 escrevendo (5 min)
   - CenÃ¡rio C: 25 clientes lendo + 75 escrevendo (5 min)
5. ğŸ”„ Limpa e repopula com 20.000 registros
6. ğŸš€ Executa **2 testes** comparativos (CenÃ¡rio A com 20k)
7. ğŸ“ˆ Gera **8 relatÃ³rios HTML** + resumo comparativo

---

## ğŸ“‚ Resultados Gerados

```
results/
â””â”€â”€ trabalho_<timestamp>/
    â”œâ”€â”€ 50k_postgres_cenario-a-50-50_report/index.html
    â”œâ”€â”€ 50k_mongo_cenario-a-50-50_report/index.html
    â”œâ”€â”€ 50k_postgres_cenario-b-75-25_report/index.html
    â”œâ”€â”€ 50k_mongo_cenario-b-75-25_report/index.html
    â”œâ”€â”€ 50k_postgres_cenario-c-25-75_report/index.html
    â”œâ”€â”€ 50k_mongo_cenario-c-25-75_report/index.html
    â”œâ”€â”€ 20k_postgres_cenario-a-50-50_report/index.html
    â”œâ”€â”€ 20k_mongo_cenario-a-50-50_report/index.html
    â””â”€â”€ RESUMO.txt  â† ComparaÃ§Ã£o de mÃ©tricas
```

---

## ğŸ¯ MÃ©tricas para Analisar

| MÃ©trica | O que Ã© | Melhor |
|---------|---------|--------|
| **Throughput** | RequisiÃ§Ãµes/segundo | â¬†ï¸ MAIOR |
| **LatÃªncia MÃ©dia** | Tempo mÃ©dio de resposta | â¬‡ï¸ MENOR |
| **P95/P99** | 95%/99% das requisiÃ§Ãµes | â¬‡ï¸ MENOR |
| **Taxa de Erro** | % de falhas | â¬‡ï¸ 0% |

---

## ğŸ’¡ Dicas Importantes

### Para Teste RÃ¡pido (validar funcionamento)
```powershell
.\run-trabalho-completo.ps1 -QuickTest
# Usa apenas 1000 registros e 1 min por teste
```

### Se JÃ¡ Tiver Dados Populados
```powershell
.\run-trabalho-completo.ps1 -SkipSeed
# Pula etapa de populaÃ§Ã£o
```

### JMeter em Local Diferente
```powershell
.\run-trabalho-completo.ps1 -JMeterPath "C:\caminho\jmeter.bat"
```

### Problemas com APIs
```powershell
# Ver status
docker ps

# Reiniciar
cd api-postgres
docker-compose restart

cd ..\api-mongo
docker-compose restart
```

---

## ğŸ“ Perguntas do Trabalho

### 1. Qual banco teve melhor desempenho?
- Compare throughput e latÃªncia nos 3 cenÃ¡rios
- Analise os relatÃ³rios HTML lado a lado

### 2. Como o dataset afeta o desempenho?
- Compare mÃ©tricas: 50k vs 20k registros
- Use o arquivo RESUMO.txt

### 3. Quando usar SQL vs NoSQL?
- Baseado nos resultados dos testes
- Considere: consistÃªncia, flexibilidade, performance

---

## âœ… Checklist de ExecuÃ§Ã£o

- [ ] Docker Desktop rodando
- [ ] JMeter instalado em `C:\apache-jmeter\`
- [ ] APIs Postgres (3000) e Mongo (3001) online
- [ ] Executei `.\run-trabalho-completo.ps1`
- [ ] Aguardei conclusÃ£o (~50-60 min)
- [ ] Verifiquei 8 relatÃ³rios HTML gerados
- [ ] Li o arquivo RESUMO.txt
- [ ] Analisei os grÃ¡ficos nos relatÃ³rios
- [ ] Documentei conclusÃµes

---

## ğŸ†˜ Problemas Comuns

### "JMeter nÃ£o encontrado"
**SoluÃ§Ã£o:** Baixe em https://jmeter.apache.org e extraia para `C:\apache-jmeter\`

### "API nÃ£o responde"
**SoluÃ§Ã£o:** `docker-compose up -d` na pasta da API

### "Seed muito lento"
**Normal para 50k!** Use `-QuickTest` primeiro para validar

### "Muitos erros nos testes"
**SoluÃ§Ãµes:**
- Reduza clientes (ex: 50 ao invÃ©s de 100)
- Aumente RAM do Docker (Settings â†’ Resources)
- Use duraÃ§Ã£o maior para estabilizar

---

## ğŸ“ Comandos Ãšteis

```powershell
# Ver containers rodando
docker ps

# Ver logs
docker logs api-postgres-app-1
docker logs api-mongo-app-1

# Testar API manualmente
Invoke-RestMethod -Uri "http://localhost:3000/produtos"

# Ver resultados
ls ..\results

# Abrir relatÃ³rio
start ..\results\trabalho_<timestamp>\50k_postgres_cenario-a-50-50_report\index.html

# Limpar tudo
docker-compose down -v  # Execute em cada pasta de API
```

---

## ğŸ“ Para o RelatÃ³rio do Trabalho

### Estrutura Sugerida

1. **IntroduÃ§Ã£o**
   - Objetivo: Comparar SQL vs NoSQL
   - Tecnologias: Nest.js, PostgreSQL, MongoDB, JMeter

2. **Metodologia**
   - Arquitetura (3 camadas)
   - Dataset: 50k produtos
   - CenÃ¡rios: A (50/50), B (75/25), C (25/75)
   - Clientes: 100 simultÃ¢neos

3. **Resultados**
   - Tabela comparativa (todas as mÃ©tricas)
   - GrÃ¡ficos dos relatÃ³rios HTML
   - ComparaÃ§Ã£o 50k vs 20k

4. **AnÃ¡lise**
   - Qual banco foi melhor em cada cenÃ¡rio?
   - Por quÃª?
   - Impacto do tamanho do dataset

5. **ConclusÃ£o**
   - Quando usar cada tecnologia
   - LiÃ§Ãµes aprendidas

### EvidÃªncias para Anexar
- Screenshots dos relatÃ³rios HTML (Dashboard)
- Arquivo RESUMO.txt
- GrÃ¡ficos de Response Time e Throughput
- Tabela comparativa preenchida

---

**ğŸ‰ Tudo pronto! Execute `.\run-trabalho-completo.ps1` e aguarde os resultados!**

Leia `TRABALHO.md` para detalhes completos.
